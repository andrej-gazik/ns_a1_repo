{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from config import config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchviz import make_dot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading, scaling the data\n",
    "data has been explored and concaternated in `data_preparerationipynb`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/train.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Separate the target variable from the features\n",
    "y = df['price_range'].values\n",
    "X = df.drop('price_range', axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "#  Split the data into train, validation, and test sets 0.7, 0.15, 0.15\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=config['seed'])\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=config['seed'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class PhoneDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # Convert inputs to torch 32 bit float tensor\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        # One hot encode the labels\n",
    "        self.enc = OneHotEncoder(sparse=False)\n",
    "        y = self.enc.fit_transform(y.reshape(-1, 1))\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < 0 or idx >= len(self):\n",
    "            raise IndexError(f\"Index {idx} is out of range\")\n",
    "\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# how to evaluate model with wandb\n",
    "def train(args, model, train_loader, optimizer, criterion):\n",
    "    # Switch model to training mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct_train = 0\n",
    "\n",
    "    # We loop over the data iterator, and feed the inputs to the network and adjust the weights.\n",
    "    for inputs, targets in train_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Feed the inputs to the network\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        output_category_train = np.argmax(outputs.detach().numpy(), axis=1)\n",
    "        target_category_train = np.argmax(targets.detach().numpy(), axis=1)\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backpropagate the gradient\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute the loss sum up batch loss\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct_train += (output_category_train == target_category_train).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy_train = 100. * correct_train / len(train_loader.dataset)\n",
    "    wandb.log({'train_loss': train_loss, 'train_accuracy': accuracy_train})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def test(args, model, criterion, test_loader):\n",
    "    # Switch model to evaluation mode. This is necessary for layers like dropout, batchnorm etc which behave differently in training and evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Load the input features and labels from the test dataset\n",
    "\n",
    "            output = model(data)\n",
    "            output_category = np.argmax(output, axis=1)\n",
    "            target_category = np.argmax(target, axis=1)\n",
    "\n",
    "            # Compute the loss sum up batch loss\n",
    "            test_loss += criterion(output, target).item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            correct += (output_category == target_category).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        wandb.log({'test_loss': test_loss, 'test_accuracy': accuracy})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Forward feed neural network\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dims=config['hidden_layers'], output_dim=4, dropout_prob=config['dropout_fix']):\n",
    "        super(FFNN, self).__init__()\n",
    "        if hidden_dims[0] is not 0:\n",
    "            self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "            self.hidden_layers = nn.ModuleList([nn.Linear(hidden_dims[i], hidden_dims[i+1]) for i in range(len(hidden_dims)-1)])\n",
    "            self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "        else:\n",
    "            self.input_layer = nn.Linear(input_dim, output_dim)\n",
    "            self.hidden_layers = []\n",
    "            self.output_layer = nn.Linear(output_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "    def init_weights(self):\n",
    "        for layer in self.hidden_layers:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "        nn.init.xavier_uniform_(self.output_layer.weight)\n",
    "\n",
    "    def init_bias(self):\n",
    "        for layer in self.hidden_layers:\n",
    "            nn.init.zeros_(layer.bias)\n",
    "        nn.init.zeros_(self.output_layer.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.input_layer(x))\n",
    "        x = self.dropout(x)\n",
    "        # Check if hidden_layers is in this class\n",
    "        for layer in self.hidden_layers:\n",
    "            x = nn.functional.relu(layer(x))\n",
    "            x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = nn.functional.softmax(x, dim=1)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Config"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sweep initialization\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    global model_name\n",
    "    global model_dims\n",
    "\n",
    "    wandb.init(project=\"phone-price-prediction\", name=model_name, config=config)\n",
    "\n",
    "    # Create the datasets\n",
    "    train_dataset = PhoneDataset(X_train, y_train)\n",
    "    val_dataset = PhoneDataset(X_val, y_val)\n",
    "    test_dataset = PhoneDataset(X_test, y_test)\n",
    "\n",
    "    # Create the data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=wandb.config.batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=wandb.config.batch_size)\n",
    "\n",
    "    # Define the device as cpu (training only on cpu)\n",
    "    device  = torch.device(\"cpu\")\n",
    "\n",
    "    # Set the seed\n",
    "    torch.manual_seed(config['seed'])\n",
    "\n",
    "    # New intialization of the model with dimensions specified in config file\n",
    "    model = FFNN(hidden_dims=model_dims, dropout_prob=wandb.config.dropout)\n",
    "\n",
    "    # Initialize the weights and bias\n",
    "    model.init_weights()\n",
    "    model.init_bias()\n",
    "\n",
    "    # Log the model architecture as an image\n",
    "    dot = make_dot(model(torch.randn(1, 20)), params=dict(model.named_parameters()))\n",
    "    dot.render('images/model', format='png')\n",
    "    image = wandb.Image('images/model.png')\n",
    "\n",
    "    # Log the model architecture as an image\n",
    "    wandb.log({'model_image': image})\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    loss = config['loss_pytorch']\n",
    "    optimizer = optim.Adam(model.parameters(), lr=wandb.config.lr, betas=(config['beta1'], config['beta2']), eps=config['epsilon'])\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(1, wandb.config.epochs + 1):\n",
    "        train(config, model, train_loader, optimizer, loss)\n",
    "        test(config, model, loss , val_loader)\n",
    "    wandb.finish()\n",
    "\n",
    "# Loop over predefined architectures and run the sweep agent\n",
    "for layer in config['hidden_layers']:\n",
    "\n",
    "    name = 'PyTorch_' + str(layer)\n",
    "    # Global variables for name and dims workaround\n",
    "    # Not the best solution but works for now :)\n",
    "    global model_name\n",
    "    global model_dims\n",
    "\n",
    "    # Set global for each run of architecture\n",
    "    model_name = name\n",
    "    model_dims = layer\n",
    "    # Sweep for each of the architectures\n",
    "    sweep_id = wandb.sweep(sweep=config, project=\"phone-price-prediction\")\n",
    "    wandb.agent(sweep_id, function=main, count = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NN Architecture"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_dataset = PhoneDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Get one batch of training data\n",
    "X, y = next(iter(train_loader))\n",
    "print(X.shape, y.shape)\n",
    "model = FFNN()\n",
    "y = model(X)\n",
    "\n",
    "y = model(X)\n",
    "make_dot(y, params=dict(model.named_parameters()))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
